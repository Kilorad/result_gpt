{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c8d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "E:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.JPIJNSWNNAN3CE6LLI5FWSPHUT2VXMTH.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "E:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#сделали поддержку большого датасета\n",
    "#\n",
    "#здесь мы выгружаем GPT-2 и пытаемся дообучать на кроссдоменном датасете\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, PreTrainedTokenizer\n",
    "import codecs\n",
    "import numpy as np\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import deque\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "import result_gpt_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401438c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install trl\n",
    "#!pip install accelerate -U\n",
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Модель, с которой стартовал я: \"gpt2-medium\"\n",
    "#Модель после обучения у меня называется: \"gpt2_finetuned_3\"\n",
    "#Можно использовать более легковесную модель \"gpt2\"\n",
    "\n",
    "#model_name = \"gpt2-medium\"\n",
    "model_name = \"gpt2_finetuned_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e86ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2save = \"gpt2_finetuned_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ca349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load a pretrained model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#специальные токены\n",
    "T_OUT = tokenizer.encode('<OUT>')[0]\n",
    "T_END = tokenizer.encode('<END>')[0]\n",
    "T_PAD = tokenizer.encode('|PAD|')[0]\n",
    "T_124 = tokenizer.encode('<124>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Здесь добавляем токены ревордов\n",
    "reward_to_substr = {}\n",
    "rmax = 25\n",
    "rmin = -25\n",
    "step = 0.1\n",
    "for r in np.arange(rmin, rmax, step):\n",
    "    r = np.round(r, 2)\n",
    "    str_r = f'<r{r}>'\n",
    "    reward_to_substr[r] = str_r\n",
    "substr_to_reward = {v: k for k, v in reward_to_substr.items()}\n",
    "print('substr_to_reward', substr_to_reward)\n",
    "print('reward_to_substr', reward_to_substr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Учим эту модель.\n",
    "#Модификация токенайзера \n",
    "#добавляем несколько токенов нашей разметки\n",
    "\n",
    "#сюда надо фигануть весь словарь картинок\n",
    "video_tokens_cnt = 1024\n",
    "video_tokens = []\n",
    "for i in range(video_tokens_cnt):\n",
    "    video_tokens.append(f'<{i}>')\n",
    "    \n",
    "#и токены ревордов тоже добавить\n",
    "    \n",
    "special_tokens_dict = {'additional_special_tokens': video_tokens + ['<IN>','<OUT>','<END>','|PAD|'] + list(reward_to_substr.values())}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#print(tokenizer.all_special_tokens)\n",
    "tokenizer.pad_token = '|PAD|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abada781",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max_token = tokenizer.encode(list(reward_to_substr.values())[-1])[0]\n",
    "r_min_token = tokenizer.encode(list(reward_to_substr.values())[0])[0]\n",
    "out_token = tokenizer.encode('<OUT>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960212f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_mapping {1231:0.1, 1547:1.2} и так далее.\n",
    "#маппинг с номеров токенов на реворды\n",
    "r_mapping = {}\n",
    "for substr in list(reward_to_substr.values()):\n",
    "    r_token = tokenizer.encode(substr)[0]\n",
    "    r_mapping[r_token] = substr_to_reward[substr]\n",
    "print(r_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mem', result_gpt_res.getmem())\n",
    "model = result_gpt_res.AutoModelForCausalLMWithValueHeadCL.from_pretrained(model_name)\n",
    "#r_mapping {1231:0.1, 1547:1.2} и так далее.\n",
    "#маппинг с номеров токенов на реворды\n",
    "model.r_mapping = r_mapping\n",
    "#токен <OUT>, с него начинается прогноз\n",
    "model.out_token = out_token\n",
    "\n",
    "model._modules['pretrained_model'].resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12825f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling#,LineByLineTextDataset\n",
    "#Загрузить датасет\n",
    "print('mem', result_gpt_res.getmem())\n",
    "\n",
    "batch_size = 2\n",
    "block_size = 150\n",
    "#block_size -  размер контекста,который мы учитываем при обучении. \n",
    "#Он меньше, чем максимально доступный трансформеру.\n",
    "#Чем меньше, тем меньше расход GPU на обучении\n",
    "\n",
    "train_dataset = result_gpt_res.LineByLineTextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          #file_path='data/all_txt_cut_r.txt',\n",
    "          file_path='data/all_txt_cut_r_low.txt',#ЭТО МЕЛКИЙ ДАТАСЕТ, ЧИСТО ДЛЯ ПРОВЕРКИ ПРЦЕДУРЫ ОБУЧЕНИЯ\n",
    "          block_size=block_size)\n",
    "print(\"data loaded\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mem', result_gpt_res.getmem())\n",
    "# если не лезет в память, можно уменьшать **per_device_train_batch_size**\n",
    "# gradient_accumulation_steps лучше повышать со временем. То есть вначале вы обучаете модель на датасете, имея\n",
    "#gradient_accumulation_steps = 2, так обучение идёт быстрее всего. Затем утыкаетесь в оптимум, стопаете, сохраняетесь (дальше)\n",
    "#удваиваете gradient_accumulation_steps\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/gpt_bot\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=80000, # number of training epochs\n",
    "    per_device_train_batch_size= batch_size, # batch size for training\n",
    "    gradient_accumulation_steps=2,\n",
    "    #per_device_eval_batch_size=50,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    warmup_steps=200,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(#result_gpt_res.TrainerUnsampled(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    #train_dataset=tokenized_dataset['train'],\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=test_dataset,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12105950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # save it locally\n",
    "        model.save_pretrained(model_name2save)\n",
    "        tokenizer.save_pretrained(model_name2save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones([122, 122, 1024], device=model.v_head[0].weight.device)\n",
    "print('ones', model.v_head(ones)[0,0,0:3])\n",
    "#0.2236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66334cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#засейвить модель\n",
    "if 1:\n",
    "    # save it locally\n",
    "    model.save_pretrained(model_name2save)\n",
    "    tokenizer.save_pretrained(model_name2save)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#проверить, что модель вообще генерит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(text):\n",
    "    #text = '\\n' + text\n",
    "    inpt = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    inpt= inpt.cuda()\n",
    "    print(text)\n",
    "\n",
    "    #out = model.generate(inpt,  max_length=len(inpt[0])+300, do_sample=True, top_k=5, top_p=0.95, temperature=1, eos_token_id=T_END, pad_token_id=T_PAD)\n",
    "    out = model.generate(inpt,  max_length=len(inpt[0])+300, do_sample=False, top_k=1, top_p=0.95, temperature=0.0001, eos_token_id=T_END, pad_token_id=T_PAD)\n",
    "\n",
    "    out_tokens = torch.where(out[0]==T_OUT)\n",
    "    #last_repl = out[0][out_tokens[0][-1]+1:-1]\n",
    "    #print(out)\n",
    "    last_repl = out[0][out_tokens[0][-1]+1:-1]\n",
    "    repl = tokenizer.decode(last_repl)\n",
    "\n",
    "    print(repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf42598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a11\n",
    "text = '<916><61><922><922><922><922><922><922><55><906><1022><830><376><830><830><830><830><61><771><402><438><751><830><925><925><830><61><580><519><920><462><830><925><925><925><61><120><519><255><327><61><925><328><55><483><657><255><49><328><830><925><147><120><287><328><830><328><61><830><706><957><29><222><925><55><55><55><328><851><590><596><164><376><61><328><328><328><801><925><422><255> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n",
    "text = '<916><61><922><922><922><922><922><922><55><906><1022><830><376><830><830><830><830><61><771><402><438><751><830><925><925><830><61><580><519><920><462><830><925><925><925><61><120><519><255><327><61><925><328><55><483><657><255><49><328><830><925><147><120><287><328><830><328><61><830><706><957><29><222><925><55><55><55><328><851><590><596><164><376><61><328><328><328><801><925><422><255> save hp plan:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b81dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<705><993><941><325><418><751><937><666><681><110><418><623><72><577><261><358><313><325><974><701><884><414><564><414><63><450><624><176><806><894><564><966><626><957><638><749><66><893><924><634><595><143><120><433><202><556><328><910><57><63><692><189><294><414><751><812><49><909><125><797><261><681><503><552><761><382><657><92><343><671><473><761><505><894><857><788><499><937><334><459><746> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<32><794><685><585><276><610><578><414><769><129><450><681><966><813><250><256><862><749><438><639><551><54><794><887><500><681><520><996><957><202><848><110><985><164><54><807><520><382><508><473><725><358><110><325><597><54><937><49><473><684><156><481><417><498><718><746><404><455><745><725><363><666><325><560><176><848><438><140><172><848><473><714><872><737><120><477><136><813><893><120><746> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
