{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7c8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сделали поддержку большого датасета\n",
    "#\n",
    "#здесь мы выгружаем GPT-2 и пытаемся дообучать на кроссдоменном датасете\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, PreTrainedTokenizer\n",
    "import codecs\n",
    "import numpy as np\n",
    "import os, pickle, random\n",
    "import psutil\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import deque\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import result_gpt_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d00d205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A100 80GB PCIe'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0459a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with codecs.open('data/all_txt_cut_r.txt', 'r', 'utf8') as f:\n",
    "#    lines=f.readlines(f)\n",
    "#print(len(lines))\n",
    "#del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239bc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install trl\n",
    "#!pip install accelerate -U\n",
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0730b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_cash = 'ints_cash.bin'\n",
    "#with open(path_cash, encoding=\"utf-8\") as f:\n",
    "#    lines = f.readlines()\n",
    "#lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc6bee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(60*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6b5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trl - это для RL\n",
    "#transformers - это для предобучения\n",
    "#вначале предобучить, затем сконвертировать!\n",
    "#model_name = \"gpt2-medium\"\n",
    "#model_name = \"gpt2_finetuned_2\"\n",
    "model_name = \"gpt2_finetuned_3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04ca349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load a pretrained model\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#model = AutoModelForCausalLM.from_config(model.config)\n",
    "#model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "\n",
    "#model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4dd5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#специальные токены { vertical-output: true }\n",
    "T_OUT = tokenizer.encode('<OUT>')[0]\n",
    "T_END = tokenizer.encode('<END>')[0]\n",
    "T_PAD = tokenizer.encode('|PAD|')[0]\n",
    "T_124 = tokenizer.encode('<124>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16a022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substr_to_reward {'<r-25.0>': -25.0, '<r-24.9>': -24.9, '<r-24.8>': -24.8, '<r-24.7>': -24.7, '<r-24.6>': -24.6, '<r-24.5>': -24.5, '<r-24.4>': -24.4, '<r-24.3>': -24.3, '<r-24.2>': -24.2, '<r-24.1>': -24.1, '<r-24.0>': -24.0, '<r-23.9>': -23.9, '<r-23.8>': -23.8, '<r-23.7>': -23.7, '<r-23.6>': -23.6, '<r-23.5>': -23.5, '<r-23.4>': -23.4, '<r-23.3>': -23.3, '<r-23.2>': -23.2, '<r-23.1>': -23.1, '<r-23.0>': -23.0, '<r-22.9>': -22.9, '<r-22.8>': -22.8, '<r-22.7>': -22.7, '<r-22.6>': -22.6, '<r-22.5>': -22.5, '<r-22.4>': -22.4, '<r-22.3>': -22.3, '<r-22.2>': -22.2, '<r-22.1>': -22.1, '<r-22.0>': -22.0, '<r-21.9>': -21.9, '<r-21.8>': -21.8, '<r-21.7>': -21.7, '<r-21.6>': -21.6, '<r-21.5>': -21.5, '<r-21.4>': -21.4, '<r-21.3>': -21.3, '<r-21.2>': -21.2, '<r-21.1>': -21.1, '<r-21.0>': -21.0, '<r-20.9>': -20.9, '<r-20.8>': -20.8, '<r-20.7>': -20.7, '<r-20.6>': -20.6, '<r-20.5>': -20.5, '<r-20.4>': -20.4, '<r-20.3>': -20.3, '<r-20.2>': -20.2, '<r-20.1>': -20.1, '<r-20.0>': -20.0, '<r-19.9>': -19.9, '<r-19.8>': -19.8, '<r-19.7>': -19.7, '<r-19.6>': -19.6, '<r-19.5>': -19.5, '<r-19.4>': -19.4, '<r-19.3>': -19.3, '<r-19.2>': -19.2, '<r-19.1>': -19.1, '<r-19.0>': -19.0, '<r-18.9>': -18.9, '<r-18.8>': -18.8, '<r-18.7>': -18.7, '<r-18.6>': -18.6, '<r-18.5>': -18.5, '<r-18.4>': -18.4, '<r-18.3>': -18.3, '<r-18.2>': -18.2, '<r-18.1>': -18.1, '<r-18.0>': -18.0, '<r-17.9>': -17.9, '<r-17.8>': -17.8, '<r-17.7>': -17.7, '<r-17.6>': -17.6, '<r-17.5>': -17.5, '<r-17.4>': -17.4, '<r-17.3>': -17.3, '<r-17.2>': -17.2, '<r-17.1>': -17.1, '<r-17.0>': -17.0, '<r-16.9>': -16.9, '<r-16.8>': -16.8, '<r-16.7>': -16.7, '<r-16.6>': -16.6, '<r-16.5>': -16.5, '<r-16.4>': -16.4, '<r-16.3>': -16.3, '<r-16.2>': -16.2, '<r-16.1>': -16.1, '<r-16.0>': -16.0, '<r-15.9>': -15.9, '<r-15.8>': -15.8, '<r-15.7>': -15.7, '<r-15.6>': -15.6, '<r-15.5>': -15.5, '<r-15.4>': -15.4, '<r-15.3>': -15.3, '<r-15.2>': -15.2, '<r-15.1>': -15.1, '<r-15.0>': -15.0, '<r-14.9>': -14.9, '<r-14.8>': -14.8, '<r-14.7>': -14.7, '<r-14.6>': -14.6, '<r-14.5>': -14.5, '<r-14.4>': -14.4, '<r-14.3>': -14.3, '<r-14.2>': -14.2, '<r-14.1>': -14.1, '<r-14.0>': -14.0, '<r-13.9>': -13.9, '<r-13.8>': -13.8, '<r-13.7>': -13.7, '<r-13.6>': -13.6, '<r-13.5>': -13.5, '<r-13.4>': -13.4, '<r-13.3>': -13.3, '<r-13.2>': -13.2, '<r-13.1>': -13.1, '<r-13.0>': -13.0, '<r-12.9>': -12.9, '<r-12.8>': -12.8, '<r-12.7>': -12.7, '<r-12.6>': -12.6, '<r-12.5>': -12.5, '<r-12.4>': -12.4, '<r-12.3>': -12.3, '<r-12.2>': -12.2, '<r-12.1>': -12.1, '<r-12.0>': -12.0, '<r-11.9>': -11.9, '<r-11.8>': -11.8, '<r-11.7>': -11.7, '<r-11.6>': -11.6, '<r-11.5>': -11.5, '<r-11.4>': -11.4, '<r-11.3>': -11.3, '<r-11.2>': -11.2, '<r-11.1>': -11.1, '<r-11.0>': -11.0, '<r-10.9>': -10.9, '<r-10.8>': -10.8, '<r-10.7>': -10.7, '<r-10.6>': -10.6, '<r-10.5>': -10.5, '<r-10.4>': -10.4, '<r-10.3>': -10.3, '<r-10.2>': -10.2, '<r-10.1>': -10.1, '<r-10.0>': -10.0, '<r-9.9>': -9.9, '<r-9.8>': -9.8, '<r-9.7>': -9.7, '<r-9.6>': -9.6, '<r-9.5>': -9.5, '<r-9.4>': -9.4, '<r-9.3>': -9.3, '<r-9.2>': -9.2, '<r-9.1>': -9.1, '<r-9.0>': -9.0, '<r-8.9>': -8.9, '<r-8.8>': -8.8, '<r-8.7>': -8.7, '<r-8.6>': -8.6, '<r-8.5>': -8.5, '<r-8.4>': -8.4, '<r-8.3>': -8.3, '<r-8.2>': -8.2, '<r-8.1>': -8.1, '<r-8.0>': -8.0, '<r-7.9>': -7.9, '<r-7.8>': -7.8, '<r-7.7>': -7.7, '<r-7.6>': -7.6, '<r-7.5>': -7.5, '<r-7.4>': -7.4, '<r-7.3>': -7.3, '<r-7.2>': -7.2, '<r-7.1>': -7.1, '<r-7.0>': -7.0, '<r-6.9>': -6.9, '<r-6.8>': -6.8, '<r-6.7>': -6.7, '<r-6.6>': -6.6, '<r-6.5>': -6.5, '<r-6.4>': -6.4, '<r-6.3>': -6.3, '<r-6.2>': -6.2, '<r-6.1>': -6.1, '<r-6.0>': -6.0, '<r-5.9>': -5.9, '<r-5.8>': -5.8, '<r-5.7>': -5.7, '<r-5.6>': -5.6, '<r-5.5>': -5.5, '<r-5.4>': -5.4, '<r-5.3>': -5.3, '<r-5.2>': -5.2, '<r-5.1>': -5.1, '<r-5.0>': -5.0, '<r-4.9>': -4.9, '<r-4.8>': -4.8, '<r-4.7>': -4.7, '<r-4.6>': -4.6, '<r-4.5>': -4.5, '<r-4.4>': -4.4, '<r-4.3>': -4.3, '<r-4.2>': -4.2, '<r-4.1>': -4.1, '<r-4.0>': -4.0, '<r-3.9>': -3.9, '<r-3.8>': -3.8, '<r-3.7>': -3.7, '<r-3.6>': -3.6, '<r-3.5>': -3.5, '<r-3.4>': -3.4, '<r-3.3>': -3.3, '<r-3.2>': -3.2, '<r-3.1>': -3.1, '<r-3.0>': -3.0, '<r-2.9>': -2.9, '<r-2.8>': -2.8, '<r-2.7>': -2.7, '<r-2.6>': -2.6, '<r-2.5>': -2.5, '<r-2.4>': -2.4, '<r-2.3>': -2.3, '<r-2.2>': -2.2, '<r-2.1>': -2.1, '<r-2.0>': -2.0, '<r-1.9>': -1.9, '<r-1.8>': -1.8, '<r-1.7>': -1.7, '<r-1.6>': -1.6, '<r-1.5>': -1.5, '<r-1.4>': -1.4, '<r-1.3>': -1.3, '<r-1.2>': -1.2, '<r-1.1>': -1.1, '<r-1.0>': -1.0, '<r-0.9>': -0.9, '<r-0.8>': -0.8, '<r-0.7>': -0.7, '<r-0.6>': -0.6, '<r-0.5>': -0.5, '<r-0.4>': -0.4, '<r-0.3>': -0.3, '<r-0.2>': -0.2, '<r-0.1>': -0.1, '<r0.0>': 0.0, '<r0.1>': 0.1, '<r0.2>': 0.2, '<r0.3>': 0.3, '<r0.4>': 0.4, '<r0.5>': 0.5, '<r0.6>': 0.6, '<r0.7>': 0.7, '<r0.8>': 0.8, '<r0.9>': 0.9, '<r1.0>': 1.0, '<r1.1>': 1.1, '<r1.2>': 1.2, '<r1.3>': 1.3, '<r1.4>': 1.4, '<r1.5>': 1.5, '<r1.6>': 1.6, '<r1.7>': 1.7, '<r1.8>': 1.8, '<r1.9>': 1.9, '<r2.0>': 2.0, '<r2.1>': 2.1, '<r2.2>': 2.2, '<r2.3>': 2.3, '<r2.4>': 2.4, '<r2.5>': 2.5, '<r2.6>': 2.6, '<r2.7>': 2.7, '<r2.8>': 2.8, '<r2.9>': 2.9, '<r3.0>': 3.0, '<r3.1>': 3.1, '<r3.2>': 3.2, '<r3.3>': 3.3, '<r3.4>': 3.4, '<r3.5>': 3.5, '<r3.6>': 3.6, '<r3.7>': 3.7, '<r3.8>': 3.8, '<r3.9>': 3.9, '<r4.0>': 4.0, '<r4.1>': 4.1, '<r4.2>': 4.2, '<r4.3>': 4.3, '<r4.4>': 4.4, '<r4.5>': 4.5, '<r4.6>': 4.6, '<r4.7>': 4.7, '<r4.8>': 4.8, '<r4.9>': 4.9, '<r5.0>': 5.0, '<r5.1>': 5.1, '<r5.2>': 5.2, '<r5.3>': 5.3, '<r5.4>': 5.4, '<r5.5>': 5.5, '<r5.6>': 5.6, '<r5.7>': 5.7, '<r5.8>': 5.8, '<r5.9>': 5.9, '<r6.0>': 6.0, '<r6.1>': 6.1, '<r6.2>': 6.2, '<r6.3>': 6.3, '<r6.4>': 6.4, '<r6.5>': 6.5, '<r6.6>': 6.6, '<r6.7>': 6.7, '<r6.8>': 6.8, '<r6.9>': 6.9, '<r7.0>': 7.0, '<r7.1>': 7.1, '<r7.2>': 7.2, '<r7.3>': 7.3, '<r7.4>': 7.4, '<r7.5>': 7.5, '<r7.6>': 7.6, '<r7.7>': 7.7, '<r7.8>': 7.8, '<r7.9>': 7.9, '<r8.0>': 8.0, '<r8.1>': 8.1, '<r8.2>': 8.2, '<r8.3>': 8.3, '<r8.4>': 8.4, '<r8.5>': 8.5, '<r8.6>': 8.6, '<r8.7>': 8.7, '<r8.8>': 8.8, '<r8.9>': 8.9, '<r9.0>': 9.0, '<r9.1>': 9.1, '<r9.2>': 9.2, '<r9.3>': 9.3, '<r9.4>': 9.4, '<r9.5>': 9.5, '<r9.6>': 9.6, '<r9.7>': 9.7, '<r9.8>': 9.8, '<r9.9>': 9.9, '<r10.0>': 10.0, '<r10.1>': 10.1, '<r10.2>': 10.2, '<r10.3>': 10.3, '<r10.4>': 10.4, '<r10.5>': 10.5, '<r10.6>': 10.6, '<r10.7>': 10.7, '<r10.8>': 10.8, '<r10.9>': 10.9, '<r11.0>': 11.0, '<r11.1>': 11.1, '<r11.2>': 11.2, '<r11.3>': 11.3, '<r11.4>': 11.4, '<r11.5>': 11.5, '<r11.6>': 11.6, '<r11.7>': 11.7, '<r11.8>': 11.8, '<r11.9>': 11.9, '<r12.0>': 12.0, '<r12.1>': 12.1, '<r12.2>': 12.2, '<r12.3>': 12.3, '<r12.4>': 12.4, '<r12.5>': 12.5, '<r12.6>': 12.6, '<r12.7>': 12.7, '<r12.8>': 12.8, '<r12.9>': 12.9, '<r13.0>': 13.0, '<r13.1>': 13.1, '<r13.2>': 13.2, '<r13.3>': 13.3, '<r13.4>': 13.4, '<r13.5>': 13.5, '<r13.6>': 13.6, '<r13.7>': 13.7, '<r13.8>': 13.8, '<r13.9>': 13.9, '<r14.0>': 14.0, '<r14.1>': 14.1, '<r14.2>': 14.2, '<r14.3>': 14.3, '<r14.4>': 14.4, '<r14.5>': 14.5, '<r14.6>': 14.6, '<r14.7>': 14.7, '<r14.8>': 14.8, '<r14.9>': 14.9, '<r15.0>': 15.0, '<r15.1>': 15.1, '<r15.2>': 15.2, '<r15.3>': 15.3, '<r15.4>': 15.4, '<r15.5>': 15.5, '<r15.6>': 15.6, '<r15.7>': 15.7, '<r15.8>': 15.8, '<r15.9>': 15.9, '<r16.0>': 16.0, '<r16.1>': 16.1, '<r16.2>': 16.2, '<r16.3>': 16.3, '<r16.4>': 16.4, '<r16.5>': 16.5, '<r16.6>': 16.6, '<r16.7>': 16.7, '<r16.8>': 16.8, '<r16.9>': 16.9, '<r17.0>': 17.0, '<r17.1>': 17.1, '<r17.2>': 17.2, '<r17.3>': 17.3, '<r17.4>': 17.4, '<r17.5>': 17.5, '<r17.6>': 17.6, '<r17.7>': 17.7, '<r17.8>': 17.8, '<r17.9>': 17.9, '<r18.0>': 18.0, '<r18.1>': 18.1, '<r18.2>': 18.2, '<r18.3>': 18.3, '<r18.4>': 18.4, '<r18.5>': 18.5, '<r18.6>': 18.6, '<r18.7>': 18.7, '<r18.8>': 18.8, '<r18.9>': 18.9, '<r19.0>': 19.0, '<r19.1>': 19.1, '<r19.2>': 19.2, '<r19.3>': 19.3, '<r19.4>': 19.4, '<r19.5>': 19.5, '<r19.6>': 19.6, '<r19.7>': 19.7, '<r19.8>': 19.8, '<r19.9>': 19.9, '<r20.0>': 20.0, '<r20.1>': 20.1, '<r20.2>': 20.2, '<r20.3>': 20.3, '<r20.4>': 20.4, '<r20.5>': 20.5, '<r20.6>': 20.6, '<r20.7>': 20.7, '<r20.8>': 20.8, '<r20.9>': 20.9, '<r21.0>': 21.0, '<r21.1>': 21.1, '<r21.2>': 21.2, '<r21.3>': 21.3, '<r21.4>': 21.4, '<r21.5>': 21.5, '<r21.6>': 21.6, '<r21.7>': 21.7, '<r21.8>': 21.8, '<r21.9>': 21.9, '<r22.0>': 22.0, '<r22.1>': 22.1, '<r22.2>': 22.2, '<r22.3>': 22.3, '<r22.4>': 22.4, '<r22.5>': 22.5, '<r22.6>': 22.6, '<r22.7>': 22.7, '<r22.8>': 22.8, '<r22.9>': 22.9, '<r23.0>': 23.0, '<r23.1>': 23.1, '<r23.2>': 23.2, '<r23.3>': 23.3, '<r23.4>': 23.4, '<r23.5>': 23.5, '<r23.6>': 23.6, '<r23.7>': 23.7, '<r23.8>': 23.8, '<r23.9>': 23.9, '<r24.0>': 24.0, '<r24.1>': 24.1, '<r24.2>': 24.2, '<r24.3>': 24.3, '<r24.4>': 24.4, '<r24.5>': 24.5, '<r24.6>': 24.6, '<r24.7>': 24.7, '<r24.8>': 24.8, '<r24.9>': 24.9}\n",
      "reward_to_substr {-25.0: '<r-25.0>', -24.9: '<r-24.9>', -24.8: '<r-24.8>', -24.7: '<r-24.7>', -24.6: '<r-24.6>', -24.5: '<r-24.5>', -24.4: '<r-24.4>', -24.3: '<r-24.3>', -24.2: '<r-24.2>', -24.1: '<r-24.1>', -24.0: '<r-24.0>', -23.9: '<r-23.9>', -23.8: '<r-23.8>', -23.7: '<r-23.7>', -23.6: '<r-23.6>', -23.5: '<r-23.5>', -23.4: '<r-23.4>', -23.3: '<r-23.3>', -23.2: '<r-23.2>', -23.1: '<r-23.1>', -23.0: '<r-23.0>', -22.9: '<r-22.9>', -22.8: '<r-22.8>', -22.7: '<r-22.7>', -22.6: '<r-22.6>', -22.5: '<r-22.5>', -22.4: '<r-22.4>', -22.3: '<r-22.3>', -22.2: '<r-22.2>', -22.1: '<r-22.1>', -22.0: '<r-22.0>', -21.9: '<r-21.9>', -21.8: '<r-21.8>', -21.7: '<r-21.7>', -21.6: '<r-21.6>', -21.5: '<r-21.5>', -21.4: '<r-21.4>', -21.3: '<r-21.3>', -21.2: '<r-21.2>', -21.1: '<r-21.1>', -21.0: '<r-21.0>', -20.9: '<r-20.9>', -20.8: '<r-20.8>', -20.7: '<r-20.7>', -20.6: '<r-20.6>', -20.5: '<r-20.5>', -20.4: '<r-20.4>', -20.3: '<r-20.3>', -20.2: '<r-20.2>', -20.1: '<r-20.1>', -20.0: '<r-20.0>', -19.9: '<r-19.9>', -19.8: '<r-19.8>', -19.7: '<r-19.7>', -19.6: '<r-19.6>', -19.5: '<r-19.5>', -19.4: '<r-19.4>', -19.3: '<r-19.3>', -19.2: '<r-19.2>', -19.1: '<r-19.1>', -19.0: '<r-19.0>', -18.9: '<r-18.9>', -18.8: '<r-18.8>', -18.7: '<r-18.7>', -18.6: '<r-18.6>', -18.5: '<r-18.5>', -18.4: '<r-18.4>', -18.3: '<r-18.3>', -18.2: '<r-18.2>', -18.1: '<r-18.1>', -18.0: '<r-18.0>', -17.9: '<r-17.9>', -17.8: '<r-17.8>', -17.7: '<r-17.7>', -17.6: '<r-17.6>', -17.5: '<r-17.5>', -17.4: '<r-17.4>', -17.3: '<r-17.3>', -17.2: '<r-17.2>', -17.1: '<r-17.1>', -17.0: '<r-17.0>', -16.9: '<r-16.9>', -16.8: '<r-16.8>', -16.7: '<r-16.7>', -16.6: '<r-16.6>', -16.5: '<r-16.5>', -16.4: '<r-16.4>', -16.3: '<r-16.3>', -16.2: '<r-16.2>', -16.1: '<r-16.1>', -16.0: '<r-16.0>', -15.9: '<r-15.9>', -15.8: '<r-15.8>', -15.7: '<r-15.7>', -15.6: '<r-15.6>', -15.5: '<r-15.5>', -15.4: '<r-15.4>', -15.3: '<r-15.3>', -15.2: '<r-15.2>', -15.1: '<r-15.1>', -15.0: '<r-15.0>', -14.9: '<r-14.9>', -14.8: '<r-14.8>', -14.7: '<r-14.7>', -14.6: '<r-14.6>', -14.5: '<r-14.5>', -14.4: '<r-14.4>', -14.3: '<r-14.3>', -14.2: '<r-14.2>', -14.1: '<r-14.1>', -14.0: '<r-14.0>', -13.9: '<r-13.9>', -13.8: '<r-13.8>', -13.7: '<r-13.7>', -13.6: '<r-13.6>', -13.5: '<r-13.5>', -13.4: '<r-13.4>', -13.3: '<r-13.3>', -13.2: '<r-13.2>', -13.1: '<r-13.1>', -13.0: '<r-13.0>', -12.9: '<r-12.9>', -12.8: '<r-12.8>', -12.7: '<r-12.7>', -12.6: '<r-12.6>', -12.5: '<r-12.5>', -12.4: '<r-12.4>', -12.3: '<r-12.3>', -12.2: '<r-12.2>', -12.1: '<r-12.1>', -12.0: '<r-12.0>', -11.9: '<r-11.9>', -11.8: '<r-11.8>', -11.7: '<r-11.7>', -11.6: '<r-11.6>', -11.5: '<r-11.5>', -11.4: '<r-11.4>', -11.3: '<r-11.3>', -11.2: '<r-11.2>', -11.1: '<r-11.1>', -11.0: '<r-11.0>', -10.9: '<r-10.9>', -10.8: '<r-10.8>', -10.7: '<r-10.7>', -10.6: '<r-10.6>', -10.5: '<r-10.5>', -10.4: '<r-10.4>', -10.3: '<r-10.3>', -10.2: '<r-10.2>', -10.1: '<r-10.1>', -10.0: '<r-10.0>', -9.9: '<r-9.9>', -9.8: '<r-9.8>', -9.7: '<r-9.7>', -9.6: '<r-9.6>', -9.5: '<r-9.5>', -9.4: '<r-9.4>', -9.3: '<r-9.3>', -9.2: '<r-9.2>', -9.1: '<r-9.1>', -9.0: '<r-9.0>', -8.9: '<r-8.9>', -8.8: '<r-8.8>', -8.7: '<r-8.7>', -8.6: '<r-8.6>', -8.5: '<r-8.5>', -8.4: '<r-8.4>', -8.3: '<r-8.3>', -8.2: '<r-8.2>', -8.1: '<r-8.1>', -8.0: '<r-8.0>', -7.9: '<r-7.9>', -7.8: '<r-7.8>', -7.7: '<r-7.7>', -7.6: '<r-7.6>', -7.5: '<r-7.5>', -7.4: '<r-7.4>', -7.3: '<r-7.3>', -7.2: '<r-7.2>', -7.1: '<r-7.1>', -7.0: '<r-7.0>', -6.9: '<r-6.9>', -6.8: '<r-6.8>', -6.7: '<r-6.7>', -6.6: '<r-6.6>', -6.5: '<r-6.5>', -6.4: '<r-6.4>', -6.3: '<r-6.3>', -6.2: '<r-6.2>', -6.1: '<r-6.1>', -6.0: '<r-6.0>', -5.9: '<r-5.9>', -5.8: '<r-5.8>', -5.7: '<r-5.7>', -5.6: '<r-5.6>', -5.5: '<r-5.5>', -5.4: '<r-5.4>', -5.3: '<r-5.3>', -5.2: '<r-5.2>', -5.1: '<r-5.1>', -5.0: '<r-5.0>', -4.9: '<r-4.9>', -4.8: '<r-4.8>', -4.7: '<r-4.7>', -4.6: '<r-4.6>', -4.5: '<r-4.5>', -4.4: '<r-4.4>', -4.3: '<r-4.3>', -4.2: '<r-4.2>', -4.1: '<r-4.1>', -4.0: '<r-4.0>', -3.9: '<r-3.9>', -3.8: '<r-3.8>', -3.7: '<r-3.7>', -3.6: '<r-3.6>', -3.5: '<r-3.5>', -3.4: '<r-3.4>', -3.3: '<r-3.3>', -3.2: '<r-3.2>', -3.1: '<r-3.1>', -3.0: '<r-3.0>', -2.9: '<r-2.9>', -2.8: '<r-2.8>', -2.7: '<r-2.7>', -2.6: '<r-2.6>', -2.5: '<r-2.5>', -2.4: '<r-2.4>', -2.3: '<r-2.3>', -2.2: '<r-2.2>', -2.1: '<r-2.1>', -2.0: '<r-2.0>', -1.9: '<r-1.9>', -1.8: '<r-1.8>', -1.7: '<r-1.7>', -1.6: '<r-1.6>', -1.5: '<r-1.5>', -1.4: '<r-1.4>', -1.3: '<r-1.3>', -1.2: '<r-1.2>', -1.1: '<r-1.1>', -1.0: '<r-1.0>', -0.9: '<r-0.9>', -0.8: '<r-0.8>', -0.7: '<r-0.7>', -0.6: '<r-0.6>', -0.5: '<r-0.5>', -0.4: '<r-0.4>', -0.3: '<r-0.3>', -0.2: '<r-0.2>', -0.1: '<r-0.1>', 0.0: '<r0.0>', 0.1: '<r0.1>', 0.2: '<r0.2>', 0.3: '<r0.3>', 0.4: '<r0.4>', 0.5: '<r0.5>', 0.6: '<r0.6>', 0.7: '<r0.7>', 0.8: '<r0.8>', 0.9: '<r0.9>', 1.0: '<r1.0>', 1.1: '<r1.1>', 1.2: '<r1.2>', 1.3: '<r1.3>', 1.4: '<r1.4>', 1.5: '<r1.5>', 1.6: '<r1.6>', 1.7: '<r1.7>', 1.8: '<r1.8>', 1.9: '<r1.9>', 2.0: '<r2.0>', 2.1: '<r2.1>', 2.2: '<r2.2>', 2.3: '<r2.3>', 2.4: '<r2.4>', 2.5: '<r2.5>', 2.6: '<r2.6>', 2.7: '<r2.7>', 2.8: '<r2.8>', 2.9: '<r2.9>', 3.0: '<r3.0>', 3.1: '<r3.1>', 3.2: '<r3.2>', 3.3: '<r3.3>', 3.4: '<r3.4>', 3.5: '<r3.5>', 3.6: '<r3.6>', 3.7: '<r3.7>', 3.8: '<r3.8>', 3.9: '<r3.9>', 4.0: '<r4.0>', 4.1: '<r4.1>', 4.2: '<r4.2>', 4.3: '<r4.3>', 4.4: '<r4.4>', 4.5: '<r4.5>', 4.6: '<r4.6>', 4.7: '<r4.7>', 4.8: '<r4.8>', 4.9: '<r4.9>', 5.0: '<r5.0>', 5.1: '<r5.1>', 5.2: '<r5.2>', 5.3: '<r5.3>', 5.4: '<r5.4>', 5.5: '<r5.5>', 5.6: '<r5.6>', 5.7: '<r5.7>', 5.8: '<r5.8>', 5.9: '<r5.9>', 6.0: '<r6.0>', 6.1: '<r6.1>', 6.2: '<r6.2>', 6.3: '<r6.3>', 6.4: '<r6.4>', 6.5: '<r6.5>', 6.6: '<r6.6>', 6.7: '<r6.7>', 6.8: '<r6.8>', 6.9: '<r6.9>', 7.0: '<r7.0>', 7.1: '<r7.1>', 7.2: '<r7.2>', 7.3: '<r7.3>', 7.4: '<r7.4>', 7.5: '<r7.5>', 7.6: '<r7.6>', 7.7: '<r7.7>', 7.8: '<r7.8>', 7.9: '<r7.9>', 8.0: '<r8.0>', 8.1: '<r8.1>', 8.2: '<r8.2>', 8.3: '<r8.3>', 8.4: '<r8.4>', 8.5: '<r8.5>', 8.6: '<r8.6>', 8.7: '<r8.7>', 8.8: '<r8.8>', 8.9: '<r8.9>', 9.0: '<r9.0>', 9.1: '<r9.1>', 9.2: '<r9.2>', 9.3: '<r9.3>', 9.4: '<r9.4>', 9.5: '<r9.5>', 9.6: '<r9.6>', 9.7: '<r9.7>', 9.8: '<r9.8>', 9.9: '<r9.9>', 10.0: '<r10.0>', 10.1: '<r10.1>', 10.2: '<r10.2>', 10.3: '<r10.3>', 10.4: '<r10.4>', 10.5: '<r10.5>', 10.6: '<r10.6>', 10.7: '<r10.7>', 10.8: '<r10.8>', 10.9: '<r10.9>', 11.0: '<r11.0>', 11.1: '<r11.1>', 11.2: '<r11.2>', 11.3: '<r11.3>', 11.4: '<r11.4>', 11.5: '<r11.5>', 11.6: '<r11.6>', 11.7: '<r11.7>', 11.8: '<r11.8>', 11.9: '<r11.9>', 12.0: '<r12.0>', 12.1: '<r12.1>', 12.2: '<r12.2>', 12.3: '<r12.3>', 12.4: '<r12.4>', 12.5: '<r12.5>', 12.6: '<r12.6>', 12.7: '<r12.7>', 12.8: '<r12.8>', 12.9: '<r12.9>', 13.0: '<r13.0>', 13.1: '<r13.1>', 13.2: '<r13.2>', 13.3: '<r13.3>', 13.4: '<r13.4>', 13.5: '<r13.5>', 13.6: '<r13.6>', 13.7: '<r13.7>', 13.8: '<r13.8>', 13.9: '<r13.9>', 14.0: '<r14.0>', 14.1: '<r14.1>', 14.2: '<r14.2>', 14.3: '<r14.3>', 14.4: '<r14.4>', 14.5: '<r14.5>', 14.6: '<r14.6>', 14.7: '<r14.7>', 14.8: '<r14.8>', 14.9: '<r14.9>', 15.0: '<r15.0>', 15.1: '<r15.1>', 15.2: '<r15.2>', 15.3: '<r15.3>', 15.4: '<r15.4>', 15.5: '<r15.5>', 15.6: '<r15.6>', 15.7: '<r15.7>', 15.8: '<r15.8>', 15.9: '<r15.9>', 16.0: '<r16.0>', 16.1: '<r16.1>', 16.2: '<r16.2>', 16.3: '<r16.3>', 16.4: '<r16.4>', 16.5: '<r16.5>', 16.6: '<r16.6>', 16.7: '<r16.7>', 16.8: '<r16.8>', 16.9: '<r16.9>', 17.0: '<r17.0>', 17.1: '<r17.1>', 17.2: '<r17.2>', 17.3: '<r17.3>', 17.4: '<r17.4>', 17.5: '<r17.5>', 17.6: '<r17.6>', 17.7: '<r17.7>', 17.8: '<r17.8>', 17.9: '<r17.9>', 18.0: '<r18.0>', 18.1: '<r18.1>', 18.2: '<r18.2>', 18.3: '<r18.3>', 18.4: '<r18.4>', 18.5: '<r18.5>', 18.6: '<r18.6>', 18.7: '<r18.7>', 18.8: '<r18.8>', 18.9: '<r18.9>', 19.0: '<r19.0>', 19.1: '<r19.1>', 19.2: '<r19.2>', 19.3: '<r19.3>', 19.4: '<r19.4>', 19.5: '<r19.5>', 19.6: '<r19.6>', 19.7: '<r19.7>', 19.8: '<r19.8>', 19.9: '<r19.9>', 20.0: '<r20.0>', 20.1: '<r20.1>', 20.2: '<r20.2>', 20.3: '<r20.3>', 20.4: '<r20.4>', 20.5: '<r20.5>', 20.6: '<r20.6>', 20.7: '<r20.7>', 20.8: '<r20.8>', 20.9: '<r20.9>', 21.0: '<r21.0>', 21.1: '<r21.1>', 21.2: '<r21.2>', 21.3: '<r21.3>', 21.4: '<r21.4>', 21.5: '<r21.5>', 21.6: '<r21.6>', 21.7: '<r21.7>', 21.8: '<r21.8>', 21.9: '<r21.9>', 22.0: '<r22.0>', 22.1: '<r22.1>', 22.2: '<r22.2>', 22.3: '<r22.3>', 22.4: '<r22.4>', 22.5: '<r22.5>', 22.6: '<r22.6>', 22.7: '<r22.7>', 22.8: '<r22.8>', 22.9: '<r22.9>', 23.0: '<r23.0>', 23.1: '<r23.1>', 23.2: '<r23.2>', 23.3: '<r23.3>', 23.4: '<r23.4>', 23.5: '<r23.5>', 23.6: '<r23.6>', 23.7: '<r23.7>', 23.8: '<r23.8>', 23.9: '<r23.9>', 24.0: '<r24.0>', 24.1: '<r24.1>', 24.2: '<r24.2>', 24.3: '<r24.3>', 24.4: '<r24.4>', 24.5: '<r24.5>', 24.6: '<r24.6>', 24.7: '<r24.7>', 24.8: '<r24.8>', 24.9: '<r24.9>'}\n"
     ]
    }
   ],
   "source": [
    "reward_to_substr = {}\n",
    "rmax = 25\n",
    "rmin = -25\n",
    "step = 0.1\n",
    "for r in np.arange(rmin, rmax, step):\n",
    "    r = np.round(r, 2)\n",
    "    str_r = f'<r{r}>'\n",
    "    reward_to_substr[r] = str_r\n",
    "substr_to_reward = {v: k for k, v in reward_to_substr.items()}\n",
    "print('substr_to_reward', substr_to_reward)\n",
    "print('reward_to_substr', reward_to_substr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9cc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Учим эту модель. У Nehc есть пример, как это делать\n",
    "#Модификация токенайзера \n",
    "#добавляем несколько токенов нашей разметки\n",
    "\n",
    "#сюда надо фигануть весь словарь картинок\n",
    "video_tokens_cnt = 1024\n",
    "video_tokens = []\n",
    "for i in range(video_tokens_cnt):\n",
    "    video_tokens.append(f'<{i}>')\n",
    "    \n",
    "special_tokens_dict = {'additional_special_tokens': video_tokens + ['<IN>','<OUT>','<END>','|PAD|'] + list(reward_to_substr.values())}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#print(tokenizer.all_special_tokens)\n",
    "tokenizer.pad_token = '|PAD|'\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abada781",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max_token = tokenizer.encode(list(reward_to_substr.values())[-1])[0]\n",
    "r_min_token = tokenizer.encode(list(reward_to_substr.values())[0])[0]\n",
    "out_token = tokenizer.encode('<OUT>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960212f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{51685: -25.0, 51686: -24.9, 51687: -24.8, 51688: -24.7, 51689: -24.6, 51690: -24.5, 51691: -24.4, 51692: -24.3, 51693: -24.2, 51694: -24.1, 51695: -24.0, 51696: -23.9, 51697: -23.8, 51698: -23.7, 51699: -23.6, 51700: -23.5, 51701: -23.4, 51702: -23.3, 51703: -23.2, 51704: -23.1, 51705: -23.0, 51706: -22.9, 51707: -22.8, 51708: -22.7, 51709: -22.6, 51710: -22.5, 51711: -22.4, 51712: -22.3, 51713: -22.2, 51714: -22.1, 51715: -22.0, 51716: -21.9, 51717: -21.8, 51718: -21.7, 51719: -21.6, 51720: -21.5, 51721: -21.4, 51722: -21.3, 51723: -21.2, 51724: -21.1, 51725: -21.0, 51726: -20.9, 51727: -20.8, 51728: -20.7, 51729: -20.6, 51730: -20.5, 51731: -20.4, 51732: -20.3, 51733: -20.2, 51734: -20.1, 51285: -20.0, 51286: -19.9, 51287: -19.8, 51288: -19.7, 51289: -19.6, 51290: -19.5, 51291: -19.4, 51292: -19.3, 51293: -19.2, 51294: -19.1, 51295: -19.0, 51296: -18.9, 51297: -18.8, 51298: -18.7, 51299: -18.6, 51300: -18.5, 51301: -18.4, 51302: -18.3, 51303: -18.2, 51304: -18.1, 51305: -18.0, 51306: -17.9, 51307: -17.8, 51308: -17.7, 51309: -17.6, 51310: -17.5, 51311: -17.4, 51312: -17.3, 51313: -17.2, 51314: -17.1, 51315: -17.0, 51316: -16.9, 51317: -16.8, 51318: -16.7, 51319: -16.6, 51320: -16.5, 51321: -16.4, 51322: -16.3, 51323: -16.2, 51324: -16.1, 51325: -16.0, 51326: -15.9, 51327: -15.8, 51328: -15.7, 51329: -15.6, 51330: -15.5, 51331: -15.4, 51332: -15.3, 51333: -15.2, 51334: -15.1, 51335: -15.0, 51336: -14.9, 51337: -14.8, 51338: -14.7, 51339: -14.6, 51340: -14.5, 51341: -14.4, 51342: -14.3, 51343: -14.2, 51344: -14.1, 51345: -14.0, 51346: -13.9, 51347: -13.8, 51348: -13.7, 51349: -13.6, 51350: -13.5, 51351: -13.4, 51352: -13.3, 51353: -13.2, 51354: -13.1, 51355: -13.0, 51356: -12.9, 51357: -12.8, 51358: -12.7, 51359: -12.6, 51360: -12.5, 51361: -12.4, 51362: -12.3, 51363: -12.2, 51364: -12.1, 51365: -12.0, 51366: -11.9, 51367: -11.8, 51368: -11.7, 51369: -11.6, 51370: -11.5, 51371: -11.4, 51372: -11.3, 51373: -11.2, 51374: -11.1, 51375: -11.0, 51376: -10.9, 51377: -10.8, 51378: -10.7, 51379: -10.6, 51380: -10.5, 51381: -10.4, 51382: -10.3, 51383: -10.2, 51384: -10.1, 51385: -10.0, 51386: -9.9, 51387: -9.8, 51388: -9.7, 51389: -9.6, 51390: -9.5, 51391: -9.4, 51392: -9.3, 51393: -9.2, 51394: -9.1, 51395: -9.0, 51396: -8.9, 51397: -8.8, 51398: -8.7, 51399: -8.6, 51400: -8.5, 51401: -8.4, 51402: -8.3, 51403: -8.2, 51404: -8.1, 51405: -8.0, 51406: -7.9, 51407: -7.8, 51408: -7.7, 51409: -7.6, 51410: -7.5, 51411: -7.4, 51412: -7.3, 51413: -7.2, 51414: -7.1, 51415: -7.0, 51416: -6.9, 51417: -6.8, 51418: -6.7, 51419: -6.6, 51420: -6.5, 51421: -6.4, 51422: -6.3, 51423: -6.2, 51424: -6.1, 51425: -6.0, 51426: -5.9, 51427: -5.8, 51428: -5.7, 51429: -5.6, 51430: -5.5, 51431: -5.4, 51432: -5.3, 51433: -5.2, 51434: -5.1, 51435: -5.0, 51436: -4.9, 51437: -4.8, 51438: -4.7, 51439: -4.6, 51440: -4.5, 51441: -4.4, 51442: -4.3, 51443: -4.2, 51444: -4.1, 51445: -4.0, 51446: -3.9, 51447: -3.8, 51448: -3.7, 51449: -3.6, 51450: -3.5, 51451: -3.4, 51452: -3.3, 51453: -3.2, 51454: -3.1, 51455: -3.0, 51456: -2.9, 51457: -2.8, 51458: -2.7, 51459: -2.6, 51460: -2.5, 51461: -2.4, 51462: -2.3, 51463: -2.2, 51464: -2.1, 51465: -2.0, 51466: -1.9, 51467: -1.8, 51468: -1.7, 51469: -1.6, 51470: -1.5, 51471: -1.4, 51472: -1.3, 51473: -1.2, 51474: -1.1, 51475: -1.0, 51476: -0.9, 51477: -0.8, 51478: -0.7, 51479: -0.6, 51480: -0.5, 51481: -0.4, 51482: -0.3, 51483: -0.2, 51484: -0.1, 51485: 0.0, 51486: 0.1, 51487: 0.2, 51488: 0.3, 51489: 0.4, 51490: 0.5, 51491: 0.6, 51492: 0.7, 51493: 0.8, 51494: 0.9, 51495: 1.0, 51496: 1.1, 51497: 1.2, 51498: 1.3, 51499: 1.4, 51500: 1.5, 51501: 1.6, 51502: 1.7, 51503: 1.8, 51504: 1.9, 51505: 2.0, 51506: 2.1, 51507: 2.2, 51508: 2.3, 51509: 2.4, 51510: 2.5, 51511: 2.6, 51512: 2.7, 51513: 2.8, 51514: 2.9, 51515: 3.0, 51516: 3.1, 51517: 3.2, 51518: 3.3, 51519: 3.4, 51520: 3.5, 51521: 3.6, 51522: 3.7, 51523: 3.8, 51524: 3.9, 51525: 4.0, 51526: 4.1, 51527: 4.2, 51528: 4.3, 51529: 4.4, 51530: 4.5, 51531: 4.6, 51532: 4.7, 51533: 4.8, 51534: 4.9, 51535: 5.0, 51536: 5.1, 51537: 5.2, 51538: 5.3, 51539: 5.4, 51540: 5.5, 51541: 5.6, 51542: 5.7, 51543: 5.8, 51544: 5.9, 51545: 6.0, 51546: 6.1, 51547: 6.2, 51548: 6.3, 51549: 6.4, 51550: 6.5, 51551: 6.6, 51552: 6.7, 51553: 6.8, 51554: 6.9, 51555: 7.0, 51556: 7.1, 51557: 7.2, 51558: 7.3, 51559: 7.4, 51560: 7.5, 51561: 7.6, 51562: 7.7, 51563: 7.8, 51564: 7.9, 51565: 8.0, 51566: 8.1, 51567: 8.2, 51568: 8.3, 51569: 8.4, 51570: 8.5, 51571: 8.6, 51572: 8.7, 51573: 8.8, 51574: 8.9, 51575: 9.0, 51576: 9.1, 51577: 9.2, 51578: 9.3, 51579: 9.4, 51580: 9.5, 51581: 9.6, 51582: 9.7, 51583: 9.8, 51584: 9.9, 51585: 10.0, 51586: 10.1, 51587: 10.2, 51588: 10.3, 51589: 10.4, 51590: 10.5, 51591: 10.6, 51592: 10.7, 51593: 10.8, 51594: 10.9, 51595: 11.0, 51596: 11.1, 51597: 11.2, 51598: 11.3, 51599: 11.4, 51600: 11.5, 51601: 11.6, 51602: 11.7, 51603: 11.8, 51604: 11.9, 51605: 12.0, 51606: 12.1, 51607: 12.2, 51608: 12.3, 51609: 12.4, 51610: 12.5, 51611: 12.6, 51612: 12.7, 51613: 12.8, 51614: 12.9, 51615: 13.0, 51616: 13.1, 51617: 13.2, 51618: 13.3, 51619: 13.4, 51620: 13.5, 51621: 13.6, 51622: 13.7, 51623: 13.8, 51624: 13.9, 51625: 14.0, 51626: 14.1, 51627: 14.2, 51628: 14.3, 51629: 14.4, 51630: 14.5, 51631: 14.6, 51632: 14.7, 51633: 14.8, 51634: 14.9, 51635: 15.0, 51636: 15.1, 51637: 15.2, 51638: 15.3, 51639: 15.4, 51640: 15.5, 51641: 15.6, 51642: 15.7, 51643: 15.8, 51644: 15.9, 51645: 16.0, 51646: 16.1, 51647: 16.2, 51648: 16.3, 51649: 16.4, 51650: 16.5, 51651: 16.6, 51652: 16.7, 51653: 16.8, 51654: 16.9, 51655: 17.0, 51656: 17.1, 51657: 17.2, 51658: 17.3, 51659: 17.4, 51660: 17.5, 51661: 17.6, 51662: 17.7, 51663: 17.8, 51664: 17.9, 51665: 18.0, 51666: 18.1, 51667: 18.2, 51668: 18.3, 51669: 18.4, 51670: 18.5, 51671: 18.6, 51672: 18.7, 51673: 18.8, 51674: 18.9, 51675: 19.0, 51676: 19.1, 51677: 19.2, 51678: 19.3, 51679: 19.4, 51680: 19.5, 51681: 19.6, 51682: 19.7, 51683: 19.8, 51684: 19.9, 51735: 20.0, 51736: 20.1, 51737: 20.2, 51738: 20.3, 51739: 20.4, 51740: 20.5, 51741: 20.6, 51742: 20.7, 51743: 20.8, 51744: 20.9, 51745: 21.0, 51746: 21.1, 51747: 21.2, 51748: 21.3, 51749: 21.4, 51750: 21.5, 51751: 21.6, 51752: 21.7, 51753: 21.8, 51754: 21.9, 51755: 22.0, 51756: 22.1, 51757: 22.2, 51758: 22.3, 51759: 22.4, 51760: 22.5, 51761: 22.6, 51762: 22.7, 51763: 22.8, 51764: 22.9, 51765: 23.0, 51766: 23.1, 51767: 23.2, 51768: 23.3, 51769: 23.4, 51770: 23.5, 51771: 23.6, 51772: 23.7, 51773: 23.8, 51774: 23.9, 51775: 24.0, 51776: 24.1, 51777: 24.2, 51778: 24.3, 51779: 24.4, 51780: 24.5, 51781: 24.6, 51782: 24.7, 51783: 24.8, 51784: 24.9}\n"
     ]
    }
   ],
   "source": [
    "#r_mapping {1231:0.1, 1547:1.2} и так далее.\n",
    "#маппинг с номеров токенов на реворды\n",
    "r_mapping = {}\n",
    "for substr in list(reward_to_substr.values()):\n",
    "    r_token = tokenizer.encode(substr)[0]\n",
    "    r_mapping[r_token] = substr_to_reward[substr]\n",
    "print(r_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f05e2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem 5.435943603515625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2_finetuned_3 were not used when initializing GPT2LMHeadModel: ['v_head.2.bias', 'v_head.4.weight', 'v_head.4.bias', 'v_head.0.bias', 'v_head.2.weight', 'v_head.0.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 51785. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51785, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mem', result_gpt_res.getmem())\n",
    "model = result_gpt_res.AutoModelForCausalLMWithValueHeadCL.from_pretrained(model_name)\n",
    "#r_mapping {1231:0.1, 1547:1.2} и так далее.\n",
    "#маппинг с номеров токенов на реворды\n",
    "model.r_mapping = r_mapping\n",
    "#токен <OUT>, с него начинается прогноз\n",
    "model.out_token = out_token\n",
    "\n",
    "model._modules['pretrained_model'].resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "554a329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones tensor([0.2236], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones([122, 122, 1024], device=model.v_head[0].weight.device)\n",
    "print('ones', model.v_head(ones)[0,0,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12825f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem 0.00023651123046875\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling#,LineByLineTextDataset\n",
    "print('mem', result_gpt_res.getmem())\n",
    "batch_size = 124#110#128#135 is moo much\n",
    "batch_size = 61#block_size=256\n",
    "batch_size = 62\n",
    "#2.28 GB (2.3 - перебор), всё работает\n",
    "\n",
    "#train_dataset = TextIterableDataset(\n",
    "#          tokenizer=tokenizer,\n",
    "#          args={'overwrite_cache':True, 'model_type':'gpt2_medium'},\n",
    "#          file_path='data/parts/all_txt.txt',\n",
    "#          block_size=batch_size)\n",
    "train_dataset = result_gpt_res.LineByLineTextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          #file_path='data/parts/all_txt.txt',\n",
    "          #file_path='data/all_txt_low.txt',\n",
    "          file_path='data/all_txt_cut_r.txt',\n",
    "          #file_path='data/all_txt_cut_r_low.txt',\n",
    "          block_size=256)\n",
    "print(1)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#обучение\n",
    "print('mem', result_gpt_res.getmem())\n",
    "# если не лезет в память, можно уменьшать **per_device_train_batch_size**\n",
    "model.tokenizer = tokenizer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/gpt_bot\", #The output directory\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    #fp16=True,######\n",
    "    #fp16_full_eval=True,\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=800, # number of training epochs\n",
    "    per_device_train_batch_size= batch_size,#70, # batch size for training\n",
    "    gradient_accumulation_steps=80,#70,#75,#70,#12,\n",
    "    #per_device_eval_batch_size=50,  # batch size for evaluation\n",
    "    eval_steps = 3, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    warmup_steps=200,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(#result_gpt_res.TrainerUnsampled(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    #train_dataset=tokenized_dataset['train'],\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=test_dataset,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mem 0.0034456253051757812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e67e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f6aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0f814d9f404b726f1beb594c4a9edab08db81a07 #14:47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12105950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    #try:\n",
    "        trainer.train()\n",
    "    #except Exception as e:\n",
    "    #    print(e)\n",
    "        # save it locally\n",
    "    #    model.save_pretrained(\"gpt2_finetuned_3\")\n",
    "    #    tokenizer.save_pretrained('gpt2_finetuned_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones([122, 122, 1024], device=model.v_head[0].weight.device)\n",
    "print('ones', model.v_head(ones)[0,0,0:3])\n",
    "#0.2236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66334cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#можно засейвить модель и потом применять в RL\n",
    "if 1:\n",
    "    # save it locally\n",
    "    model.save_pretrained(\"gpt2_finetuned_3\")\n",
    "    tokenizer.save_pretrained('gpt2_finetuned_3')\n",
    "\n",
    "    # load the model from the Hub\n",
    "    #from transformers import AutoModelForCausalLM\n",
    "\n",
    "    #model = AutoModelForCausalLM.from_pretrained(\"my-fine-tuned-model-ppo\")\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(text):\n",
    "    #text = '\\n' + text\n",
    "    inpt = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    inpt= inpt.cuda()\n",
    "    print(text)\n",
    "\n",
    "    #out = model.generate(inpt,  max_length=len(inpt[0])+300, do_sample=True, top_k=5, top_p=0.95, temperature=1, eos_token_id=T_END, pad_token_id=T_PAD)\n",
    "    out = model.generate(inpt,  max_length=len(inpt[0])+300, do_sample=False, top_k=1, top_p=0.95, temperature=0.0001, eos_token_id=T_END, pad_token_id=T_PAD)\n",
    "\n",
    "    out_tokens = torch.where(out[0]==T_OUT)\n",
    "    #last_repl = out[0][out_tokens[0][-1]+1:-1]\n",
    "    #print(out)\n",
    "    last_repl = out[0][out_tokens[0][-1]+1:-1]\n",
    "    repl = tokenizer.decode(last_repl)\n",
    "\n",
    "    print(repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf42598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a11\n",
    "text = '<916><61><922><922><922><922><922><922><55><906><1022><830><376><830><830><830><830><61><771><402><438><751><830><925><925><830><61><580><519><920><462><830><925><925><925><61><120><519><255><327><61><925><328><55><483><657><255><49><328><830><925><147><120><287><328><830><328><61><830><706><957><29><222><925><55><55><55><328><851><590><596><164><376><61><328><328><328><801><925><422><255> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n",
    "text = '<916><61><922><922><922><922><922><922><55><906><1022><830><376><830><830><830><830><61><771><402><438><751><830><925><925><830><61><580><519><920><462><830><925><925><925><61><120><519><255><327><61><925><328><55><483><657><255><49><328><830><925><147><120><287><328><830><328><61><830><706><957><29><222><925><55><55><55><328><851><590><596><164><376><61><328><328><328><801><925><422><255> save hp plan:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b81dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<705><993><941><325><418><751><937><666><681><110><418><623><72><577><261><358><313><325><974><701><884><414><564><414><63><450><624><176><806><894><564><966><626><957><638><749><66><893><924><634><595><143><120><433><202><556><328><910><57><63><692><189><294><414><751><812><49><909><125><797><261><681><503><552><761><382><657><92><343><671><473><761><505><894><857><788><499><937><334><459><746> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<32><794><685><585><276><610><578><414><769><129><450><681><966><813><250><256><862><749><438><639><551><54><794><887><500><681><520><996><957><202><848><110><985><164><54><807><520><382><508><473><725><358><110><325><597><54><937><49><473><684><156><481><417><498><718><746><404><455><745><725><363><666><325><560><176><848><438><140><172><848><473><714><872><737><120><477><136><813><893><120><746> description:'\n",
    "text = f\"<IN>{text}<OUT>\"\n",
    "answer(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587258d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Итак, основная идея в том, что мы генерим description и пишем его после картинки, но до плана. Фишка в том, что зная description, можно уже принимать нихреновые решения. Но у нас description не отличается подробностью (1), и (2) в нём могут быть дублирующиеся объекты. Чем подробнее description, тем он дольше генерится, и тем меньше их влезет в память. Но лучше по нему можно принимать решения, с большей детальностью.\n",
    "Как его генерить эффективно?\n",
    "1) Группировать объекты. Перечислить, что у нас ahead, left-ahead, at left, top-left ahead, far left-ahead и так далее. Это можно сделать через dict из counter (чтобы не писать imp at left, imp at left, а написать сразу количество)\n",
    "2) Сделать токенами длинные, но частотные слова. Right, left, ahead, turn, какие там ещё команды будут.\n",
    "3) В п1 мы разделили экран на 9 зон по x и y (или на 18 зон, если ещё и по глубине). Можно в эти зоны добавить объекты \"стена\", \"дырка\", \"препятствие\" - их можно описать через геометрию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) +Реворды записывать для каждого кадра, в стиле sarsa (проверь! И убедись, что разные задачи записываются)\n",
    "2) -В summarize учитывать goal (потому что иначе хз, как предсказывать реворды. И убедись, что это работает в инференсе)\n",
    "3) -Инференс на CPU+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Разберись, наконец, с GPU/CPU!+\n",
    "Пойми, где лаги (в CPU режиме), убедись, что там точно нет перехода на GPU. Докажи это, блин.+\n",
    "Если проблема не решится - перейди на GPU. Выясни, опять же, где лаги.+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
